{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKBe3hJrlThT"
      },
      "source": [
        "# Finetuning Using Google Gemma's Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-5gJk3W_s0RY"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q -U bitsandbytes==0.42.0\n",
        "!pip3 install -q -U peft==0.8.2\n",
        "!pip3 install -q -U trl==0.7.10\n",
        "!pip3 install -q -U accelerate==0.27.1\n",
        "!pip3 install -q -U datasets==2.17.0\n",
        "!pip3 install -q -U transformers==4.38.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mi50mprVsU_P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import transformers\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, GemmaTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3XgwC9A4idTY"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"] = \"******\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cff6aWM0XCCn"
      },
      "source": [
        "### Prerequisites\n",
        "* nf4(4-bit NormalFloat(NF4)) : https://www.kaggle.com/code/lorentzyeung/what-s-4-bit-quantization-how-does-it-help-llama2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2B7XLisjijDM"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/gemma-2b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2da4fc9379cf4979a05db96cbd03d2ac",
            "0467e8fa4bbf42b8a133ab9a43ddf8d2",
            "2b00d38e3db145a8bdeba41d1c4ba2f6",
            "fe9423bb558f42438672312004ea5fdd",
            "712a26aee9fb41788a5b232b8e0f89df",
            "bc5e63b352504b5099e011159324caf1",
            "9751c5256f31442d93aa867d4a77cc5a",
            "d995545f88c243e4a366c7069570f1a8",
            "c404c4ae0b7d484f95553e76c88dc77a",
            "fae0f88364f5487f812a064bd04bcd34",
            "1592fb72c0b847ac8396de1af95d42d2"
          ]
        },
        "id": "EVEotZX8s-v6",
        "outputId": "3d3ad59c-20b6-40c0-a62f-0a39fe1edf63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2da4fc9379cf4979a05db96cbd03d2ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\":0},\n",
        "                                             token=os.environ['HF_TOKEN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_wABB6DloF5"
      },
      "source": [
        "### Before Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Msk610TVUGW",
        "outputId": "549f9fb3-0ac2-4552-ab76-470cc9047ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quote: The tragedy of life is not that people fail. It is that people succeed.\n",
            "\n",
            "The above quote is a great reminder that we should not be\n"
          ]
        }
      ],
      "source": [
        "text = \"Quote: The tragedy of life is not that people fail\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bciWTw7Jfz7w",
        "outputId": "1689cdb8-773b-4d1f-d5bf-b221f6ae59af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quote: All you need is a good memory and the ability to see things in their true form.\n",
            "\n",
            "The <b>Memory</b> is a character in the <i>Mega Man X</i>\n"
          ]
        }
      ],
      "source": [
        "text = \"Quote: All you need is a good memory and the ability to see things in their\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20, temperature=0.1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6C_z4TjDOiI",
        "outputId": "09838234-f497-458a-f460-f9e3c52919a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quote: When you have eliminated the impossible, whatever remains, however improbable, must be the truth.\n",
            "\n",
            "- Sir Arthur Conan Doyle\n",
            "\n",
            "The above quote is a great example of\n"
          ]
        }
      ],
      "source": [
        "text = \"Quote: When you have eliminated the impossible, whatever remains, however improbable,\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh9CzaPMlt7Q"
      },
      "source": [
        "#### The Quotes are not what we wanted, the LLM is not Trained on these Quotes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67fH8t03l2DR"
      },
      "source": [
        "## Fine-Tune Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mi2P12KsVbyt"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7lzjoG3KVRMN"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r = 8,\n",
        "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type = \"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WBWkBQkeIfWN"
      },
      "outputs": [],
      "source": [
        "with open('quotes.txt', 'r') as file:\n",
        "    data = file.read().replace('\\n', '')\n",
        "\n",
        "quotes = data.split(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "53Te1UUXo55v"
      },
      "outputs": [],
      "source": [
        "stripped_quotes = [quote.strip() for quote in quotes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xnjfgVEwpj4h"
      },
      "outputs": [],
      "source": [
        "mydata = {'quote' : stripped_quotes}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9GSpmdtzT9Ne"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(mydata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12_IcXOiUASU",
        "outputId": "bd7aee02-2bec-475e-b0fe-0d6165ac14c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['quote'],\n",
              "    num_rows: 125\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OUk5mh0ijek6"
      },
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"Quote: {example['quote'][0]}\"\n",
        "    return [text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "75eb95b696c44e089f070d3ca8341f97",
            "f7d17bda7b1441619db3257e686d292e",
            "ec50ec4ed7e54d2785691055804d7281",
            "9922c0746be74793a32bc8a600e3332b",
            "7d252d72288c439a9f17d7c60e969a97",
            "e1fd276f7a504bab8e9583c34666529a",
            "cf65ca668ae44883bbb6dcfac991bf31",
            "86361d52a74d461bbf5560a27e91c2cd",
            "20f910502724422594cf628d83186a38",
            "cdeb0180822e4ff8ab681afdb0ca5707",
            "bd25719b5ebc47a2aaa8cd97580c6341"
          ]
        },
        "id": "qZW2D4P9jjAX",
        "outputId": "c5101b85-647d-424c-f0c6-91b65f478068"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75eb95b696c44e089f070d3ca8341f97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=35,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=5,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    peft_config=lora_config,\n",
        "    formatting_func=formatting_func,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "Bk0PFuXubvIT",
        "outputId": "a02c4be1-f02e-4830-ff01-319566229bc9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:11, Epoch 35/35]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.741900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.402900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.145800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.107800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.077500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.062000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=35, training_loss=0.25188413177217756, metrics={'train_runtime': 11.7887, 'train_samples_per_second': 11.876, 'train_steps_per_second': 2.969, 'total_flos': 9201608908800.0, 'train_loss': 0.25188413177217756, 'epoch': 35.0})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikfZisf2cHMU",
        "outputId": "57e66fff-9527-4ca5-ce4f-634b4403e360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quote: The tragedy of life is not that people fail, but that they could succeed with so little effort. -Modelo T. A. Edison\n",
            "\n",
            "The\n"
          ]
        }
      ],
      "source": [
        "text = \"Quote: The tragedy of life is not that people fail\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djg3QAMuVx8R",
        "outputId": "74f1439a-5f27-439c-a93e-469c1b23b6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quote: All you need is a good memory and the ability to see things in their true colors. — <strong>George R R Martin</strong>\n",
            "The tragedy of life is not that people\n"
          ]
        }
      ],
      "source": [
        "text = \"Quote: All you need is a good memory and the ability to see things in their\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdJIhK-6mrEL",
        "outputId": "c82443b6-d8e8-46a2-ffa2-e8e454609a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quote: When you have eliminated the impossible, whatever remains, however improbable, must be the truth. — Sir Arthur Conan Doyle\n",
            "\n",
            "Quote: The tragedy of life is not that\n"
          ]
        }
      ],
      "source": [
        "text = \"Quote: When you have eliminated the impossible, whatever remains, however improbable,\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsoZD9Y9l_A5"
      },
      "source": [
        "#### LLM now accurately predict the quotes to some extent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to8NLIo3mHY5"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnji6SegiYr6",
        "outputId": "7ef8c916-3b2b-4558-d8ff-82701d9fa0e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('gemma_finetuned_tokenizer/tokenizer_config.json',\n",
              " 'gemma_finetuned_tokenizer/special_tokens_map.json',\n",
              " 'gemma_finetuned_tokenizer/tokenizer.model',\n",
              " 'gemma_finetuned_tokenizer/added_tokens.json',\n",
              " 'gemma_finetuned_tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"gemma_finetuned_model\")\n",
        "tokenizer.save_pretrained(\"gemma_finetuned_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5esYo-Aidty",
        "outputId": "e04fdc39-51bd-46ae-dd16-e58154fdae38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "Some weights of the model checkpoint at gemma_finetuned_model were not used when initializing GemmaForCausalLM: ['model.layers.0.mlp.down_proj.base_layer.weight', 'model.layers.0.mlp.down_proj.base_layer.weight.absmax', 'model.layers.0.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.0.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight', 'model.layers.0.mlp.up_proj.base_layer.weight', 'model.layers.0.mlp.up_proj.base_layer.weight.absmax', 'model.layers.0.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.base_layer.weight', 'model.layers.1.mlp.down_proj.base_layer.weight.absmax', 'model.layers.1.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.1.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.gate_proj.lora_A.default.weight', 'model.layers.1.mlp.gate_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.base_layer.weight', 'model.layers.1.mlp.up_proj.base_layer.weight.absmax', 'model.layers.1.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.base_layer.weight', 'model.layers.10.mlp.down_proj.base_layer.weight.absmax', 'model.layers.10.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.10.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.gate_proj.lora_A.default.weight', 'model.layers.10.mlp.gate_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.base_layer.weight', 'model.layers.10.mlp.up_proj.base_layer.weight.absmax', 'model.layers.10.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.base_layer.weight', 'model.layers.11.mlp.down_proj.base_layer.weight.absmax', 'model.layers.11.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.11.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.gate_proj.lora_A.default.weight', 'model.layers.11.mlp.gate_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.base_layer.weight', 'model.layers.11.mlp.up_proj.base_layer.weight.absmax', 'model.layers.11.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.base_layer.weight', 'model.layers.12.mlp.down_proj.base_layer.weight.absmax', 'model.layers.12.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.12.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.gate_proj.lora_A.default.weight', 'model.layers.12.mlp.gate_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.base_layer.weight', 'model.layers.12.mlp.up_proj.base_layer.weight.absmax', 'model.layers.12.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.base_layer.weight', 'model.layers.13.mlp.down_proj.base_layer.weight.absmax', 'model.layers.13.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.13.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.gate_proj.lora_A.default.weight', 'model.layers.13.mlp.gate_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.base_layer.weight', 'model.layers.13.mlp.up_proj.base_layer.weight.absmax', 'model.layers.13.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.base_layer.weight', 'model.layers.14.mlp.down_proj.base_layer.weight.absmax', 'model.layers.14.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.14.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.gate_proj.lora_A.default.weight', 'model.layers.14.mlp.gate_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.base_layer.weight', 'model.layers.14.mlp.up_proj.base_layer.weight.absmax', 'model.layers.14.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.base_layer.weight', 'model.layers.15.mlp.down_proj.base_layer.weight.absmax', 'model.layers.15.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.15.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.gate_proj.lora_A.default.weight', 'model.layers.15.mlp.gate_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.base_layer.weight', 'model.layers.15.mlp.up_proj.base_layer.weight.absmax', 'model.layers.15.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.base_layer.weight', 'model.layers.16.mlp.down_proj.base_layer.weight.absmax', 'model.layers.16.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.16.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.gate_proj.lora_A.default.weight', 'model.layers.16.mlp.gate_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.base_layer.weight', 'model.layers.16.mlp.up_proj.base_layer.weight.absmax', 'model.layers.16.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.base_layer.weight', 'model.layers.17.mlp.down_proj.base_layer.weight.absmax', 'model.layers.17.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.17.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.gate_proj.lora_A.default.weight', 'model.layers.17.mlp.gate_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.base_layer.weight', 'model.layers.17.mlp.up_proj.base_layer.weight.absmax', 'model.layers.17.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.base_layer.weight', 'model.layers.2.mlp.down_proj.base_layer.weight.absmax', 'model.layers.2.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.2.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.gate_proj.lora_A.default.weight', 'model.layers.2.mlp.gate_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.base_layer.weight', 'model.layers.2.mlp.up_proj.base_layer.weight.absmax', 'model.layers.2.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.base_layer.weight', 'model.layers.3.mlp.down_proj.base_layer.weight.absmax', 'model.layers.3.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.3.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.gate_proj.lora_A.default.weight', 'model.layers.3.mlp.gate_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.base_layer.weight', 'model.layers.3.mlp.up_proj.base_layer.weight.absmax', 'model.layers.3.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.base_layer.weight', 'model.layers.4.mlp.down_proj.base_layer.weight.absmax', 'model.layers.4.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.4.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.gate_proj.lora_A.default.weight', 'model.layers.4.mlp.gate_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.base_layer.weight', 'model.layers.4.mlp.up_proj.base_layer.weight.absmax', 'model.layers.4.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.base_layer.weight', 'model.layers.5.mlp.down_proj.base_layer.weight.absmax', 'model.layers.5.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.5.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.gate_proj.lora_A.default.weight', 'model.layers.5.mlp.gate_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.base_layer.weight', 'model.layers.5.mlp.up_proj.base_layer.weight.absmax', 'model.layers.5.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.base_layer.weight', 'model.layers.6.mlp.down_proj.base_layer.weight.absmax', 'model.layers.6.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.6.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.gate_proj.lora_A.default.weight', 'model.layers.6.mlp.gate_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.base_layer.weight', 'model.layers.6.mlp.up_proj.base_layer.weight.absmax', 'model.layers.6.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.base_layer.weight', 'model.layers.7.mlp.down_proj.base_layer.weight.absmax', 'model.layers.7.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.7.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.gate_proj.lora_A.default.weight', 'model.layers.7.mlp.gate_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.base_layer.weight', 'model.layers.7.mlp.up_proj.base_layer.weight.absmax', 'model.layers.7.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.base_layer.weight', 'model.layers.8.mlp.down_proj.base_layer.weight.absmax', 'model.layers.8.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.8.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.gate_proj.lora_A.default.weight', 'model.layers.8.mlp.gate_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.base_layer.weight', 'model.layers.8.mlp.up_proj.base_layer.weight.absmax', 'model.layers.8.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.base_layer.weight', 'model.layers.9.mlp.down_proj.base_layer.weight.absmax', 'model.layers.9.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.9.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.gate_proj.lora_A.default.weight', 'model.layers.9.mlp.gate_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.base_layer.weight', 'model.layers.9.mlp.up_proj.base_layer.weight.absmax', 'model.layers.9.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
            "- This IS expected if you are initializing GemmaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GemmaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GemmaForCausalLM were not initialized from the model checkpoint at gemma_finetuned_model and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, GemmaForCausalLM\n",
        "\n",
        "# Load the saved tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gemma_finetuned_tokenizer\")\n",
        "model = GemmaForCausalLM.from_pretrained(\"gemma_finetuned_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bLWwS2Xilwh",
        "outputId": "8b8da6c0-977c-417f-d61f-28ab4173afb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quote: The tragedy of life is not that people fail, but that they could succeed with so little effort. -Modelo T. A. Edison\n",
            "\n",
            "The\n"
          ]
        }
      ],
      "source": [
        "text = \"Quote: The tragedy of life is not that people fail\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av-dASO7jhpI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0467e8fa4bbf42b8a133ab9a43ddf8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc5e63b352504b5099e011159324caf1",
            "placeholder": "​",
            "style": "IPY_MODEL_9751c5256f31442d93aa867d4a77cc5a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1592fb72c0b847ac8396de1af95d42d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20f910502724422594cf628d83186a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b00d38e3db145a8bdeba41d1c4ba2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d995545f88c243e4a366c7069570f1a8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c404c4ae0b7d484f95553e76c88dc77a",
            "value": 2
          }
        },
        "2da4fc9379cf4979a05db96cbd03d2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0467e8fa4bbf42b8a133ab9a43ddf8d2",
              "IPY_MODEL_2b00d38e3db145a8bdeba41d1c4ba2f6",
              "IPY_MODEL_fe9423bb558f42438672312004ea5fdd"
            ],
            "layout": "IPY_MODEL_712a26aee9fb41788a5b232b8e0f89df"
          }
        },
        "712a26aee9fb41788a5b232b8e0f89df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75eb95b696c44e089f070d3ca8341f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7d17bda7b1441619db3257e686d292e",
              "IPY_MODEL_ec50ec4ed7e54d2785691055804d7281",
              "IPY_MODEL_9922c0746be74793a32bc8a600e3332b"
            ],
            "layout": "IPY_MODEL_7d252d72288c439a9f17d7c60e969a97"
          }
        },
        "7d252d72288c439a9f17d7c60e969a97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86361d52a74d461bbf5560a27e91c2cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9751c5256f31442d93aa867d4a77cc5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9922c0746be74793a32bc8a600e3332b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdeb0180822e4ff8ab681afdb0ca5707",
            "placeholder": "​",
            "style": "IPY_MODEL_bd25719b5ebc47a2aaa8cd97580c6341",
            "value": " 125/125 [00:00&lt;00:00, 5030.20 examples/s]"
          }
        },
        "bc5e63b352504b5099e011159324caf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd25719b5ebc47a2aaa8cd97580c6341": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c404c4ae0b7d484f95553e76c88dc77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdeb0180822e4ff8ab681afdb0ca5707": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf65ca668ae44883bbb6dcfac991bf31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d995545f88c243e4a366c7069570f1a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1fd276f7a504bab8e9583c34666529a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec50ec4ed7e54d2785691055804d7281": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86361d52a74d461bbf5560a27e91c2cd",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20f910502724422594cf628d83186a38",
            "value": 125
          }
        },
        "f7d17bda7b1441619db3257e686d292e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1fd276f7a504bab8e9583c34666529a",
            "placeholder": "​",
            "style": "IPY_MODEL_cf65ca668ae44883bbb6dcfac991bf31",
            "value": "Map: 100%"
          }
        },
        "fae0f88364f5487f812a064bd04bcd34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9423bb558f42438672312004ea5fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fae0f88364f5487f812a064bd04bcd34",
            "placeholder": "​",
            "style": "IPY_MODEL_1592fb72c0b847ac8396de1af95d42d2",
            "value": " 2/2 [00:19&lt;00:00,  7.92s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
